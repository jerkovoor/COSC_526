{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC 526 - Assignment 06\n",
    "### February 26, 2021\n",
    "---\n",
    "\n",
    "*Note: Running a cell will not rerun previous cells.  If you edit code in previous cells, you must rerun those cells.  We recommend using* `Run All` *to avoid any errors results from not rerunning previous cells.  You can find this in the menu above:* `Cell -> Run All`\n",
    "\n",
    "During the last lecture, we learned about the **Apache Spark** implementation of the **MapReduce** programming model.  In this assignment, we will use **PySpark** (the Spark Python API) to perform one of the text parsing problems that we solved in the last assignment *with the power of parallel processing*. In the previous assignment, we defined three sequential methods (i.e., `mapSequential`, `reduceSequential`, and `reduceByKeySequential`) that extend Python's `map` and `reduce` functions.  In this assignment, we will be using PySpark's parallel version of these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing PySpark\n",
    "Run the cell below to verify that your Java, Spark, and PySpark installations are successful. The cell generates a dataset of numbers (i.e., 1, 2, 3, 4, 5, 6, 7, 8, 9, and 10) and computes their sum. The expected output is 45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize(range(1,10))\n",
    "print(data.reduce(lambda x,y: x+y))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 0:\n",
    "Now that we are in Jetstream, open your completed Assignment05 and rerun it. Executing the same code on different machines is a valuable test of the portability of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:\n",
    "\n",
    "Now that we have Spark installed and running in our Jupyter Notebook environment, we can move from *sequential* text processing in Python (i.e., Assignments 4 and 5) to the *parallel* implementation in *Apache Spark*.  **In this assignment we will redo the problems from Assignment05 using Apache Spark.**  *Note that the code you wrote for the sequential version should work with the parallel version.  You will only need to adapt to using Spark's parallelized data structure, the `RDD`.* \n",
    "\n",
    "Below, we provide the functions implemented in the Assignment04 (e.g., the building blocks of analyses provided by domain scientists, that you are asked to parallelize).  Notice the one minor change from the previous assignment: the `preprocessLine` function is similar to `loadText` but it is designed to operate on a single line as opposed to an entire file.  This allows the pre-processing to occur on each line in the file in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method for reading and processing individual lines of a text file\n",
    "# Like `loadText` in assignment 5 except that it operates on a single line, rather than an entire file\n",
    "def preprocessLine(line):\n",
    "    import re\n",
    "    \n",
    "    # Remove all non-alphabet characters with a regular expression\n",
    "    text_alpha = re.sub(r'[^a-zA-Z]', ' ', line)\n",
    "\n",
    "    # Convert characters to upper-case\n",
    "    text_upper = text_alpha.upper()\n",
    "    \n",
    "    # Convert the string of text into a list of words and remove empty words\n",
    "    words = [w for w in text_upper.split(' ') if w != '']\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Return the length of a given word\n",
    "def wordLength(word):\n",
    "    return len(word)\n",
    "\n",
    "# Given a key and value, return a (key, value) pair\n",
    "def makeKeyValue(key, value=1):\n",
    "    return (key, value)\n",
    "\n",
    "# Count (reduce) the values for a given key (word length)\n",
    "def addValues(val1, val2):\n",
    "    return val1 + val2\n",
    "\n",
    "# Given a word, return an iterable of characters\n",
    "def splitWord(word):\n",
    "    return list(word)\n",
    "\n",
    "# Define a method to return position of each character\n",
    "def lettersPosition(word):\n",
    "    import numpy as np\n",
    "\n",
    "    if len(word) == 1:\n",
    "        # Base case for words of length 1\n",
    "        return [(word, np.array([1,0,0]))]\n",
    "    else:\n",
    "        # Get first and last letters\n",
    "        first, last = word[0], word[-1]\n",
    "        pos_list = [(first, np.array([1,0,0])), (last, np.array([0,0,1]))]\n",
    "\n",
    "        # Get interior letters\n",
    "        interior = word[1:-1]\n",
    "        for char in interior:\n",
    "            pos_list.append((char, np.array([0,1,0])))\n",
    "\n",
    "    return pos_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark Context and use the new preProcessLine function to import the text from The Count of Monty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark Context (sc)\n",
    "# <Define your Spark Context here>\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Create the RDD containing the text from the Count of Monte Cristo\n",
    "# HINT: ref 1\n",
    "textFile = sc.textFile(\"book_CountOfMonteCristo.txt\")\n",
    "# data = textFile.collect()\n",
    "# rdd = sc.parallelize(data)\n",
    "\n",
    "# Pre-process the lines in the RDD\n",
    "# (i.e., remove special characters, make uppercase, split into words)\n",
    "# What kind of Spark map function do you need in this situation?\n",
    "# Think about the structure of what is returned by each call to the map fuction,\n",
    "# and about what you want the final structure to be\n",
    "# HINT: ref 2 or 3\n",
    "\n",
    "words = textFile.flatMap(preprocessLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the text for word length frequency. We might expect short words to be more common than long words. But, are words of length 2 more common than words or length 3? Are words of length 3 more common that words of length 4? **Use the pre-processed text, `words`, from the previous cell to count the frequency of each word length in the text using the parallel MapReduce methods of Spark.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Length : Count\n",
      "3           : 109798\n",
      "2           :  84021\n",
      "4           :  81777\n",
      "5           :  49101\n",
      "6           :  39015\n",
      "7           :  30701\n"
     ]
    }
   ],
   "source": [
    "# Map the length of each word, using the wordLength function defined above\n",
    "# HINT: ref 3\n",
    "word_lengths = words.map(wordLength)\n",
    "    \n",
    "# Map keyvalue pairs to help count each word length, using the makeKeyValue function defined above\n",
    "word_keyvalues = word_lengths.map(makeKeyValue)\n",
    "# print(word_keyvalues[:10])\n",
    "    \n",
    "# ReduceByKey to count number of words with each length, using the addValues function defined above\n",
    "# HINT: ref 4\n",
    "word_length_counts = word_keyvalues.reduceByKey(addValues).collect()\n",
    "# print(word_length_counts[:10])\n",
    "\n",
    "# Extract the six most common word-lengths from the RDD\n",
    "# HINT: ref 5\n",
    "wl_counts_sorted = sc.parallelize(word_length_counts).top(6,key=lambda x: x[1])\n",
    "\n",
    "# Print the 6 most common word lengths\n",
    "print('Word Length : Count')\n",
    "for word_len, count in wl_counts_sorted:\n",
    "    print('{:<11d} : {:>6d}'.format(word_len, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "Word Length : Count\n",
    "3           : 109798\n",
    "2           :  84021\n",
    "4           :  81777\n",
    "5           :  49101\n",
    "6           :  39015\n",
    "7           :  30701\n",
    "```\n",
    "#### References\n",
    "- [1: textFile](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile)\n",
    "- [2: flatMap](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap)\n",
    "- [3: map](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map)\n",
    "- [4: reduceByKey](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)\n",
    "- [5: top](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:\n",
    "Analyze the text for letter frequency. If you’ve taken a crypto course and/or have seen substitution ciphers then you are probably aware that ’e’ is the most common letter used in the English language.  **Use the pre-processed text `words` to count the frequency of each letter in the text using the parallel MapReduce methods of Spark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character : Count\n",
      "E         : 258693\n",
      "T         : 180211\n",
      "A         : 165306\n",
      "O         : 156817\n",
      "I         : 142095\n",
      "N         : 137343\n"
     ]
    }
   ],
   "source": [
    "# The next two calls require you to use the map function\n",
    "# Think about which map (i.e., flatMap or Map) is most suitable\n",
    "\n",
    "\n",
    "# Map list of words to list characters, using the splitWord function defined above\n",
    "chars = words.flatMap(splitWord)\n",
    "\n",
    "# Map list of characters to list of key-value pairs, using the makeKeyValue function defined above\n",
    "char_keyvalues = chars.map(makeKeyValue)\n",
    "\n",
    "# ReduceByKey to count number of occurrences of each letter\n",
    "char_counts = char_keyvalues.reduceByKey(addValues).collect()\n",
    "\n",
    "# Extract the 6 most common characters from the RDD\n",
    "char_counts_sorted = sc.parallelize(char_counts).top(6,key=lambda x: x[1])\n",
    "\n",
    "# Print the 6 most common characters\n",
    "print('Character : Count')\n",
    "for letter, count in char_counts_sorted[:6]:\n",
    "    print('{:<9s} : {:>6d}'.format(letter, count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "Character : Count\n",
    "E         : 258693\n",
    "T         : 180211\n",
    "A         : 165306\n",
    "O         : 156817\n",
    "I         : 142095\n",
    "N         : 137343\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3:\n",
    "If we really wanted to crack a substitution cipher (or win on \"Wheel of Fortune\") then we should be aware that, although 'e' is the most common letter used in English, it may not be the most common first letter in a word. **Count the positional frequencies of each letter using the parallel MapReduce methods of Spark. Specifically, count the number of times each letter appears as the first letter in a word, as the last letter in a word, and as an interior letter in a word (i.e. a letter that is neither first nor last)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character : First | Interior |  Last\n",
      "A         : 51644 |   111686 |  1976\n",
      "B         : 18866 |     8516 |   541\n",
      "C         : 19577 |    32130 |   725\n",
      "D         : 17289 |    18613 | 58075\n",
      "E         : 10178 |   153205 | 95310\n",
      "F         : 17724 |    10618 | 16988\n"
     ]
    }
   ],
   "source": [
    "# Map the location of each character within the words\n",
    "char_positions = words.flatMap(lettersPosition)\n",
    "\n",
    "# Reduce the letter positions for each character\n",
    "char_position_counts = char_positions.reduceByKey(addValues).collect()\n",
    "\n",
    "# Collect the counts for the first 6 characters in the alphabet (i.e., A-F)\n",
    "# HINT: ref 1\n",
    "cp_sorted = sc.parallelize(char_position_counts).takeOrdered(6,key=lambda x: x[0])\n",
    "\n",
    "# Print the position frequency of the first letters in the alphabet\n",
    "print('Character : First | Interior |  Last')\n",
    "for char, char_position in cp_sorted[:6]:\n",
    "    first, interior, last = char_position\n",
    "    print('{:<9} : {:5d} | {:>8d} | {:>5d}'.format(char, first, interior, last))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "Character : First | Interior |  Last\n",
    "A         : 51644 |   111686 |  1976\n",
    "B         : 18866 |     8516 |   541\n",
    "C         : 19577 |    32130 |   725\n",
    "D         : 17289 |    18613 | 58075\n",
    "E         : 10178 |   153205 | 95310\n",
    "F         : 17724 |    10618 | 16988\n",
    "```\n",
    "#### References\n",
    "- [1: takeOrdered](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeOrdered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4:\n",
    "As you did in Problem 4 of Assignment04, use matplotlib to create histograms for Problems 1-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib's pyplot\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot word_length_counts from Problem 1, ordered by word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the data from the RDD into a list\n",
    "# HINT: ref[1]\n",
    "\n",
    "\n",
    "# Sort word length list by length of word\n",
    "\n",
    "\n",
    "# Get X and Y values\n",
    "\n",
    "\n",
    "# Plot the histogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot char_counts from Problem 2, ordered alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you clear (i.e., delete) previous figures with this\n",
    "plt.clf()\n",
    "\n",
    "# Collect the data from the RDD into a list\n",
    "\n",
    "\n",
    "# Sort character count list alphabetically\n",
    "\n",
    "\n",
    "# Use the sorted list of tuples contain characters and character counts to get X and Y values\n",
    "\n",
    "\n",
    "# Plot the histogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot char_position_counts from Problem 3, ordered alphabetically with three bars per letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous figure\n",
    "\n",
    "\n",
    "# Collect the data from the RDD into a list\n",
    "\n",
    "\n",
    "# Sort the list of character position count values alphabetically\n",
    "\n",
    "\n",
    "# Get a list of X values and 3 lists for Y values (first, interior, and last)\n",
    "\n",
    "\n",
    "# Set proper X_pos values for each list of values (first, interior, last)\n",
    "\n",
    "\n",
    "# Plot the histograms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- [1: collect](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5:\n",
    "**Find an interesting dataset.**\n",
    "\n",
    "Now that you've heard a bit about the projects we will be doing later in this class, find a dataset that you *could* use for the project. It should be large enough to allow for interesting analysis and non-trivial results. You don't have to download the data; make sure you do NOT add it to your GitHub repository.\n",
    "\n",
    "In the box below, describe the dataset you have selected in one or two paragraphs. Include its source (with url); how the data were collected; significance of the data; the number of rows, objects, or data points; what information is contained in each; data types (int, str, char, float, etc.) and numerical ranges where appropriate; and any details about the file that would be necessary for loading the data into a program.\n",
    "\n",
    "**NOTE: You will not have to use this dataset for your project! This is an exercise in finding and describing data for research.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Consider:\n",
    "* You used the collect() function for Problem 4 to move the data from RDDs to lists. This collects the distributed data, which is okay for the small datasets used presently, but would be unwise if you are using Spark to handle very large datasets.\n",
    "* Parallel computation can save time by completing multiple pieces of work simultaneously on different processors. However, if you were to track execution time for Problems 1-3 above and compare it with Assignment05, Assignment06 could very well be slower. Why?\n",
    "* When our dataset was a text, the questions we asked included \"How often does each word-length occur?\" and \"How often does each letter occur?\" Consider what questions you could ask about the dataset you described in Problem 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Questions:\n",
    "**Answer the following questions, in a couple sentences each, in the cells provided below**\n",
    "* List the key tasks you accomplished during this assignment?\n",
    "* Describe the challenges you faced in addressing these tasks and how you overcame these challenges?\n",
    "* Did you work with other students on this assignment? If yes, how did you help them? How did they help you? Be as specific as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
